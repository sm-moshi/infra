# NetBox Deployment Plan

**Status:** Planning
**Target:** apps/user/netbox
**Dependencies:** CNPG (cnpg-main), Valkey (shared), Proxmox CSI
**Created:** 2026-01-19

## Overview

Deploy NetBox IPAM/DCIM using wrapper chart pattern with shared infrastructure:

- **PostgreSQL:** CNPG cluster (cnpg-main-rw.apps.svc.cluster.local)
- **Redis:** Shared Valkey instance (2 databases: tasks=0, cache=1)
- **Storage:** MinIO S3 for media/reports (netbox-media bucket)
- **High Availability:** 2 replicas with pod anti-affinity across pve-01/pve-02
- **Upstream Chart:** netbox v7.3.2 from oci://ghcr.io/netbox-community/netbox-chart

### High Availability Design

**2-Replica Architecture:**

- Each replica runs on a different Proxmox host (pve-01, pve-02)
- Pod anti-affinity enforced via `topologyKey: kubernetes.io/hostname`
- S3 storage (MinIO) allows both replicas to access same media files
- If one host fails, remaining replica continues serving requests
- No ReadWriteMany volume needed - S3 provides shared access
- SSD-backed MinIO storage ensures fast media read/write performance

**Benefits:**

- ✅ Service survives single host failure
- ✅ No downtime during host maintenance/reboots
- ✅ Load distributed across both replicas
- ✅ Leverages existing MinIO deployment (no new infrastructure)
- ✅ Storage layer (MinIO) is also HA with replicas on both hosts
- ✅ No NFS or CIFS dependencies
- ✅ S3 API provides better multi-writer concurrency than filesystem

**Storage Architecture:**

- MinIO backend: Proxmox CSI `k8s-minio-ssd` storage class
- ZFS datasets: `rpool/k8s/minio` on both pve-01 and pve-02
- Settings: 1M recordsize (optimal for large media files), zstd compression
- Each MinIO replica can serve reads locally, writes replicated between hosts

### Chart Analysis

Researched actual chart values via `helm show values netbox/netbox`:

- Supports external PostgreSQL via `externalDatabase` config
- Supports external Redis/Valkey via `tasksDatabase` + `cachingDatabase` (2 separate DB indices)
- Bundled PostgreSQL + Valkey can be disabled
- Our Valkey instance has auth disabled - chart supports this configuration
- Requires `superuser.existingSecret` for admin credentials
- Persistence needed for media files

### Alternatives Considered

**netbox-operator chart (rejected):**

- Adds CRD complexity for managing NetBox resources
- Does not match existing deployment patterns (Harbor, Gitea, Semaphore use direct charts)
- Additional operator overhead without clear benefit for single-instance deployment

---

## Phase 1: Prerequisites & CNPG Configuration

### Step 1.1: Enable NetBox role in CNPG

Edit [apps/cluster/cloudnative-pg/values.yaml](../../apps/cluster/cloudnative-pg/values.yaml):

```yaml
roles:
  - name: netbox
    enabled: true
    comment: NetBox IPAM/DCIM application
    login: true
    passwordSecret:
      name: netbox-postgres-auth
```

Bump chart version: `0.2.26` → `0.2.27`

**Why first:** Creates PostgreSQL user before NetBox starts, avoiding race condition where app fails to connect.

### Step 1.2: Generate Database Password

```bash
openssl rand -base64 32
```

Save for sealed secret creation (Step 3.1).

### Step 1.3: Generate Superuser Credentials

```bash
# Admin password
openssl rand -base64 24

# API token
openssl rand -base64 32

# Django SECRET_KEY
openssl rand -base64 48
```

Save all three for sealed secret creation (Step 3.3).

### Step 1.4: Create MinIO Bucket for NetBox

**Storage Backend:** MinIO cluster using SSD-backed Proxmox CSI storage (`k8s-minio-ssd` on rpool/k8s/minio, ZFS datasets on pve-01 and pve-02 with 1M recordsize, zstd compression).

**Create bucket via MinIO console or CLI:**

```bash
# Option 1: Via kubectl port-forward + mc CLI
kubectl port-forward -n apps svc/minio 9000:9000

mc alias set local http://localhost:9000 <MINIO-ROOT-USER> <MINIO-ROOT-PASSWORD>
mc mb local/netbox-media
mc policy set download local/netbox-media  # Optional: public read for static media

# Option 2: Via MinIO Console UI
kubectl port-forward -n apps svc/minio 9001:9001
# Navigate to http://localhost:9001
# Login with root credentials from sealed secret
# Buckets → Create Bucket → Name: netbox-media
```

**Generate dedicated S3 credentials:**

```bash
# Via MinIO Console: Identity → Service Accounts → Create Service Account
# Description: netbox-media-access
# Policy: readwrite (default)
# Save Access Key and Secret Key for Step 3.2
```

**Why:** NetBox needs S3 storage for media/reports to support 2 replicas without ReadWriteMany volumes. SSD-backed storage ensures fast media access.

---

## Phase 2: Wrapper Chart Structure

### Step 2.1: Create Directory Structure

```bash
mkdir -p apps/user/netbox/{templates,charts}
```

Expected structure:

```text
apps/user/netbox/
├── Chart.yaml
├── values.yaml
├── Chart.lock              # Generated by helm dependency update
├── charts/                 # Generated - contains netbox-7.3.2.tgz
└── templates/
    ├── netbox-postgres-auth.sealedsecret.yaml
    ├── netbox-superuser.sealedsecret.yaml
    └── (additional resources as needed)
```

### Step 2.2: Create Chart.yaml

File: `apps/user/netbox/Chart.yaml`

```yaml
apiVersion: v2
name: netbox
description: Wrapper chart for NetBox IPAM/DCIM
version: 0.1.0
type: application
appVersion: "v4.5.0"
icon: https://raw.githubusercontent.com/netbox-community/netbox/main/docs/netbox_logo_light.svg
dependencies:
  - name: netbox
    version: 7.3.2
    repository: oci://ghcr.io/netbox-community/netbox-chart
```

### Step 2.3: Create values.yaml

File: `apps/user/netbox/values.yaml`

```yaml
# apps/user/netbox/values.yaml
#
# NetBox IPAM/DCIM using shared infrastructure (CNPG + Valkey)

netbox:
  # Disable bundled databases
  postgresql:
    enabled: false

  valkey:
    enabled: false

  # External PostgreSQL (CNPG cluster)
  externalDatabase:
    host: cnpg-main-rw.apps.svc.cluster.local
    port: 5432
    database: netbox
    username: netbox
    password: ""  # Provided via existingSecret
    existingSecretName: netbox-postgres-auth
    existingSecretKey: password
    engine: django.db.backends.postgresql
    connMaxAge: 300
    disableServerSideCursors: false
    options:
      sslmode: prefer
      target_session_attrs: read-write

  # External Valkey for task queue (database 0)
  tasksDatabase:
    host: valkey.apps.svc.cluster.local
    port: 6379
    database: 0
    ssl: false
    username: ""  # Auth disabled on Valkey
    password: ""
    existingSecretName: ""

  # External Valkey for caching (database 1)
  cachingDatabase:
    host: valkey.apps.svc.cluster.local
    port: 6379
    database: 1
    ssl: false
    username: ""
    password: ""
    existingSecretName: ""

  # Superuser credentials
  superuser:
    name: admin
    email: admin@m0sh1.cc
    password: ""  # Provided via existingSecret
    apiToken: ""  # Provided via existingSecret
    existingSecret: netbox-superuser

  # Security
  allowedHosts:
    - netbox.m0sh1.cc

  allowedHostsIncludesPodIP: true

  csrf:
    cookieName: csrftoken
    trustedOrigins:
      - https://netbox.m0sh1.cc

  # Storage Backend: MinIO S3 (enables multi-replica HA)
  # NetBox supports S3 via django-storages (django-storages[s3])
  # This allows 2 replicas without needing ReadWriteMany volumes
  # SSD-backed storage (k8s-minio-ssd) for fast media access
  storageBackend: storages.backends.s3boto3.S3Boto3Storage

  # S3 Configuration (MinIO in apps namespace)
  extraEnvs:
    - name: STORAGE_BACKEND
      value: "storages.backends.s3boto3.S3Boto3Storage"
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: netbox-s3-credentials
          key: access-key
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: netbox-s3-credentials
          key: secret-key
    - name: AWS_STORAGE_BUCKET_NAME
      value: "netbox-media"
    - name: AWS_S3_ENDPOINT_URL
      value: "http://minio.apps.svc.cluster.local:9000"
    - name: AWS_S3_REGION_NAME
      value: "us-east-1"  # MinIO default region
    - name: AWS_S3_USE_SSL
      value: "false"  # Internal cluster communication
    - name: AWS_S3_SIGNATURE_VERSION
      value: "s3v4"  # MinIO compatibility

  # Disable local persistence (using S3 instead)
  persistence:
    enabled: false

  # Replica count for HA (2 replicas across 2 hosts)
  replicaCount: 2

  # Resources
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2
      memory: 2Gi

  # Tolerations for control-plane scheduling
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

  # Pod Anti-Affinity: Spread replicas across different Proxmox hosts
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - netbox
          topologyKey: kubernetes.io/hostname

  # Ingress via Traefik
  ingress:
    enabled: true
    className: traefik
    hosts:
      - host: netbox.m0sh1.cc
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: netbox-tls
        hosts:
          - netbox.m0sh1.cc
```

---

## Phase 3: SealedSecrets Creation

### Step 3.1: Create netbox-postgres-auth Unsealed Template

**File:** `/tmp/netbox-postgres-auth-unsealed.yaml` (NEVER COMMIT)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: netbox-postgres-auth
  namespace: apps
type: Opaque
stringData:
  password: "<GENERATED-DB-PASSWORD-FROM-STEP-1.2>"
```

### Step 3.2: Create netbox-s3-credentials Unsealed Template

**File:** `/tmp/netbox-s3-credentials-unsealed.yaml` (NEVER COMMIT)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: netbox-s3-credentials
  namespace: apps
type: Opaque
stringData:
  access-key: "<MINIO-ACCESS-KEY-FROM-STEP-1.4>"
  secret-key: "<MINIO-SECRET-KEY-FROM-STEP-1.4>"
```

### Step 3.3: Create netbox-superuser Unsealed Template

**File:** `/tmp/netbox-superuser-unsealed.yaml` (NEVER COMMIT)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: netbox-superuser
  namespace: apps
type: Opaque
stringData:
  superuser_password: "<ADMIN-PASSWORD-FROM-STEP-1.3>"
  superuser_api_token: "<API-TOKEN-FROM-STEP-1.3>"
  secret_key: "<DJANGO-SECRET-KEY-FROM-STEP-1.3>"
```

### Step 3.4: Seal All Secrets

```bash
# Seal database credentials
kubeseal --format yaml \
  --controller-namespace=sealed-secrets \
  --controller-name=sealed-secrets-controller \
  < /tmp/netbox-postgres-auth-unsealed.yaml \
  > apps/user/netbox/templates/netbox-postgres-auth.sealedsecret.yaml

# Seal S3 credentials
kubeseal --format yaml \
  --controller-namespace=sealed-secrets \
  --controller-name=sealed-secrets-controller \
  < /tmp/netbox-s3-credentials-unsealed.yaml \
  > apps/user/netbox/templates/netbox-s3-credentials.sealedsecret.yaml

# Seal superuser credentials
kubeseal --format yaml \
  --controller-namespace=sealed-secrets \
  --controller-name=sealed-secrets-controller \
  < /tmp/netbox-superuser-unsealed.yaml \
  > apps/user/netbox/templates/netbox-superuser.sealedsecret.yaml
```

### Step 3.5: Clean Up Unsealed Files

```bash
rm /tmp/netbox-*-unsealed.yaml
```

**Security Note:** Never commit unsealed secrets. Only SealedSecrets go into Git.

---

## Phase 4: Helm Dependency Management

### Step 4.1: Update Helm Dependencies

```bash
cd apps/user/netbox/
helm dependency update
```

**Expected output:**

- Downloads `netbox-7.3.2.tgz` to `charts/`
- Creates `Chart.lock` with dependency hashes

### Step 4.2: Validate Chart

```bash
# Lint wrapper chart
helm lint apps/user/netbox/

# Render and validate manifests
helm template netbox apps/user/netbox/ | kubeconform -

# Check for issues
mise run k8s-lint
```

**Expected:** No errors, valid Kubernetes manifests generated.

---

## Phase 5: ArgoCD Application

### Step 5.1: Create ArgoCD Application Manifest

File: `argocd/apps/user/netbox.yaml`

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: netbox
  namespace: argocd
  labels:
    app.kubernetes.io/part-of: apps-root
    app.kubernetes.io/name: netbox
spec:
  project: default

  destination:
    server: https://kubernetes.default.svc
    namespace: apps

  source:
    repoURL: https://github.com/sm-moshi/infra.git
    targetRevision: main
    path: apps/user/netbox

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

---

## Phase 6: Deployment & Validation

### Step 6.1: Commit Changes

```bash
git add apps/cluster/cloudnative-pg/
git add apps/user/netbox/
git add argocd/apps/user/netbox.yaml
git commit -m "feat(netbox): add NetBox IPAM/DCIM wrapper chart

- Enable netbox role in CNPG cluster (0.2.27)
- Create wrapper chart with external PostgreSQL and Valkey
- Use shared infrastructure (cnpg-main + valkey)
- Configure persistence for media files (20Gi)
- Add sealed secrets for database and superuser
- Chart version: 0.1.0, upstream: netbox 7.3.2"
```

### Step 6.2: Push and Sync CNPG First

```bash
git push

# Wait for ArgoCD to sync cloudnative-pg
watch kubectl get application -n argocd cloudnative-pg

# Verify role created
kubectl get cluster -n apps cnpg-main -o yaml | grep -A10 "managedRoles"
```

**Expected:** `netbox` role appears in managed roles list.

### Step 6.3: Verify NetBox Database User Created

```bash
kubectl exec -n apps cnpg-main-1 -- \
  psql -U postgres -c "\du netbox"
```

**Expected output:**

```text
                 Role name | Attributes
-----------+------------------------------------------------------------
 netbox    | Create DB
```

### Step 6.4: Create NetBox Database

```bash
kubectl exec -n apps cnpg-main-1 -- \
  psql -U postgres -c "CREATE DATABASE netbox OWNER netbox;"
```

**Expected:** Database `netbox` created successfully.

### Step 6.5: Sync NetBox Application

ArgoCD will auto-sync, or manually trigger:

```bash
argocd app sync netbox --grpc-web
```

### Step 6.6: Monitor Deployment

```bash
# Watch pod creation
kubectl get pods -n apps -l app.kubernetes.io/name=netbox -w

# Follow logs
kubectl logs -n apps -l app.kubernetes.io/name=netbox --tail=100 -f
```

**Expected logs:**

```text
Checking database server...
Database available
Running migrations...
Operations to perform: Apply all migrations...
Applying contenttypes.0001_initial... OK
Applying auth.0001_initial... OK
...
Creating superuser 'admin'...
Superuser created successfully
Starting NGINX Unit...
```

### Step 6.7: Check Database Connectivity

Look for successful PostgreSQL connection in logs:

```text
PostgreSQL connection successful: netbox@cnpg-main-rw.apps.svc.cluster.local:5432/netbox
```

### Step 6.8: Verify Valkey Connectivity

Check logs for Redis/Valkey connection success:

```text
Successfully connected to Redis cache (database 1)
Successfully connected to Redis queue (database 0)
```

---

## Phase 7: Post-Deployment Validation

### Step 7.1: Check Database Schema

```bash
kubectl exec -n apps cnpg-main-1 -- \
  psql -U netbox -d netbox -c "\dt" | head -n 20
```

**Expected:** NetBox tables visible (dcim_*, ipam_*, circuits_*, etc.)

### Step 7.2: Access NetBox UI

```bash
kubectl get ingress -n apps netbox
```

**Expected output:**

```text
NAME     CLASS     HOSTS               ADDRESS         PORTS
netbox   traefik   netbox.m0sh1.cc     10.0.0.x        80, 443
```

Navigate to: `https://netbox.m0sh1.cc`

### Step 7.3: Login with Superuser

- **URL:** <https://netbox.m0sh1.cc>
- **Username:** `admin`
- **Password:** `<superuser_password from sealed secret>`

**Expected:** Successful login to NetBox admin interface.

### Step 7.4: Verify API Token

1. Navigate to: Admin → Users → admin
2. Click "API Tokens" tab
3. Pre-created token should be visible

**Test API access:**

```bash
TOKEN="<api-token-from-sealed-secret>"
curl -H "Authorization: Token $TOKEN" \
  https://netbox.m0sh1.cc/api/
```

**Expected:** JSON response with API version and endpoints.

### Step 7.5: Test S3 Media Storage

**Upload a test image:**

1. Navigate to any device or site
2. Scroll to "Images" section
3. Upload a test image file
4. Verify image displays correctly

**Verify S3 storage backend:**

```bash
# Check object was created in MinIO
kubectl port-forward -n apps svc/minio 9000:9000
mc ls local/netbox-media/

# Should show uploaded files
```

**Test multi-replica access:**

```bash
# Get both pod names
kubectl get pods -n apps -l app.kubernetes.io/name=netbox

# Delete one pod
kubectl delete pod -n apps <netbox-pod-1>

# Access UI immediately via other pod
# Navigate to uploaded image - should still display
# This confirms S3 provides shared access across replicas
```

### Step 7.6: Test Basic Functionality

**Create a site:**

1. Organization → Sites → Add
2. Fill in name and slug
3. Save

**Create a device:**

1. Devices → Devices → Add
2. Select site, device type, role
3. Save

**Assign an IP address:**

1. IPAM → IP Addresses → Add
2. Enter address, assign to device
3. Save

### Step 7.7: Verify Persistence

```bash
# Delete both pods to simulate host failure
kubectl delete pod -n apps -l app.kubernetes.io/name=netbox

# Wait for recreation (may reschedule to different nodes)
kubectl wait --for=condition=ready pod \
  -n apps -l app.kubernetes.io/name=netbox --timeout=120s

# Verify data persisted
# Login again and check:
# - Sites/devices still exist (PostgreSQL persistence)
# - Uploaded images still display (S3 persistence)
# - Both replicas can access same media files
```

---

## Phase 8: Documentation & Memory Bank

### Step 8.1: Update Memory Bank

```bash
# Log successful deployment
memory_bank_log_decision

# Update progress
memory_bank_update_progress
```

### Step 8.2: Access Documentation

**NetBox Access:**

- **URL:** <https://netbox.m0sh1.cc>
- **Admin User:** admin (email: <admin@m0sh1.cc>)
- **Credentials:** SealedSecret `netbox-superuser` in `apps` namespace

**Database Access:**

- **Service:** cnpg-main-rw.apps.svc.cluster.local:5432
- **Database:** netbox
- **User:** netbox
- **Credentials:** SealedSecret `netbox-postgres-auth` in `apps` namespace

**API Integration:**

- **Endpoint:** <https://netbox.m0sh1.cc/api/>
- **Token:** Stored in `netbox-superuser` SealedSecret
- **Documentation:** <https://netbox.m0sh1.cc/api/docs/>

---

## Critical Success Factors

### Configuration Best Practices

**Environment Variables:**

- Use explicit environment variables for S3 configuration (don't rely on config file discovery)
- Each S3 setting should be explicitly set via `extraEnvs` (AWS_S3_ENDPOINT_URL, AWS_ACCESS_KEY_ID, etc.)
- Avoid relying on application defaults that may use different S3 client behavior
- Test S3 connectivity before deploying full application

**Secrets Management:**

- Keep S3 credentials separate from database credentials (different SealedSecrets)
- Use Kubernetes secret references in env vars, not direct values in config files
- Verify secrets are unsealed after ArgoCD sync: `kubectl get secret -n apps <name> -o yaml`

### Order of Operations

**MUST follow this sequence:**

1. ✅ Enable `netbox` role in CNPG
2. ✅ Sync CNPG → verify role created
3. ✅ Create sealed secrets
4. ✅ Commit wrapper chart + ArgoCD Application
5. ✅ Sync NetBox application

**Why:** CNPG role MUST exist before NetBox starts, otherwise database connection fails.

### Validation Checkpoints

- **After CNPG sync:** Verify `netbox` user exists in PostgreSQL
- **After NetBox sync:** Check pod logs for successful migrations
- **After first login:** Verify superuser token is active
- **After creating test data:** Restart pod and verify persistence

### Common Issues & Solutions

**Issue:** Pod stuck in CrashLoopBackOff

- **Check:** Logs for "could not connect to database"
- **Solution:** Verify `netbox` database exists in PostgreSQL

**Issue:** Login fails with "invalid credentials"

- **Check:** SealedSecret was properly unsealed
- **Solution:** Verify secret exists: `kubectl get secret -n apps netbox-superuser -o yaml`

**Issue:** "DisallowedHost" error in logs

- **Check:** Ingress hostname matches `allowedHosts` in values.yaml
- **Solution:** Update values.yaml and re-sync

**Issue:** Media uploads fail / "S3 connection error"

- **Check MinIO service:** `kubectl get pods -n apps -l app.kubernetes.io/name=minio`
- **Check bucket exists:**

  ```bash
  kubectl port-forward -n apps svc/minio 9000:9000
  mc alias set local http://localhost:9000 <USER> <PASSWORD>
  mc ls local/netbox-media
  ```

- **Check S3 credentials are valid:**

  ```bash
  # Verify secret was unsealed
  kubectl get secret -n apps netbox-s3-credentials -o yaml

  # Test credentials against MinIO
  kubectl port-forward -n apps svc/minio 9000:9000
  mc alias set test http://localhost:9000 \
    $(kubectl get secret -n apps netbox-s3-credentials -o jsonpath='{.data.access-key}' | base64 -d) \
    $(kubectl get secret -n apps netbox-s3-credentials -o jsonpath='{.data.secret-key}' | base64 -d)
  mc ls test/netbox-media
  ```

- **Verify endpoint URL:**

  ```bash
  # Check NetBox sees correct endpoint
  kubectl exec -n apps deployment/netbox -- env | grep AWS_S3_ENDPOINT
  # Should show: http://minio.apps.svc.cluster.local:9000

  # Test DNS resolution from NetBox pod
  kubectl exec -n apps deployment/netbox -- nslookup minio.apps.svc.cluster.local
  ```

- **Check MinIO access policy:**

  ```bash
  # Verify service account has readwrite access
  mc admin user info local/ <access-key>
  ```

- **Debug with NetBox shell:**

  ```bash
  kubectl exec -it -n apps deployment/netbox -- python manage.py shell
  # In Python shell:
  >>> from storages.backends.s3boto3 import S3Boto3Storage
  >>> storage = S3Boto3Storage()
  >>> storage.listdir('')  # Should list bucket contents
  ```

- **Common errors:**
  - `NoSuchBucket`: Bucket doesn't exist, create via mc or console
  - `SignatureDoesNotMatch`: Wrong secret key in sealed secret
  - `InvalidAccessKeyId`: Wrong access key or service account deleted
  - `Connection refused`: Wrong endpoint URL or MinIO not running
  - `SSL certificate verification failed`: Check AWS_S3_USE_SSL=false for internal

**Issue:** Pods not spreading across hosts

- **Check:** Pod anti-affinity configuration in values.yaml
- **Solution:** Verify `topologyKey: kubernetes.io/hostname` is set
- **Verify:** `kubectl get pods -n apps -o wide` shows replicas on different nodes

**Issue:** One replica down after host reboot

- **Expected:** This is normal - second replica continues serving traffic
- **Action:** Wait for host to come back online, pod will reschedule automatically
- **Verify HA:** Test by manually cordoning one host and confirming service availability

---

## Rollback Plan

### Disable NetBox Application

```bash
# Move to disabled directory
git mv argocd/apps/user/netbox.yaml argocd/disabled/user/

# Commit
git commit -m "chore(netbox): disable NetBox application"
git push
```

**Result:** ArgoCD will delete all NetBox resources (pods, services, ingress)

### Preserve Data

**Database:**

- Database `netbox` in CNPG cluster remains intact
- Can be restored by re-enabling application

**Media Files:**

- PVC `netbox-media` remains in cluster
- Data preserved even if application is disabled

**Secrets:**

- SealedSecrets remain in Git
- Unsealed secrets remain in cluster

### Complete Removal

**If data loss is acceptable:**

```bash
# Delete database
kubectl exec -n apps cnpg-main-1 -- \
  psql -U postgres -c "DROP DATABASE netbox;"

# Delete PVC
kubectl delete pvc -n apps netbox-media

# Remove CNPG role
# Edit apps/cluster/cloudnative-pg/values.yaml
# Set netbox.enabled: false
# Bump chart version and sync
```

---

## References

- **Upstream Chart:** <https://artifacthub.io/packages/helm/netbox/netbox>
- **NetBox Docs:** <https://netboxlabs.com/docs/netbox/>
- **NetBox GitHub:** <https://github.com/netbox-community/netbox>
- **CNPG Operator:** <https://cloudnative-pg.io/>
- **Valkey (Redis):** <https://valkey.io/>

---

**Document Status:** Planning Complete - Ready for Implementation
**Last Updated:** 2026-01-19
**Approver:** GitOps team
