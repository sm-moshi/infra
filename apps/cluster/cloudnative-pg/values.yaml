# apps/cluster/cloudnative-pg/values.yaml
#
# Installs:
# - CloudNativePG operator (cluster-wide)
# - Barman Cloud plugin (CNPG-I) for S3-compatible backups/WAL
# - Shared CNPG Cluster + roles + Database CRs in namespace "apps"

cnpg:
  cluster:
    enabled: true
    name: cnpg-main
    namespace: apps

    instances: 1
    # CNPG cluster operand image.
    #
    # Important: this is NOT the operator image. `spec.imageName` must point to
    # a CloudNativePG "operand" PostgreSQL image that CNPG can run inside the
    # cluster Pods (upstream examples use `ghcr.io/cloudnative-pg/postgresql:*`).
    # Do not use Docker Official `postgres:*` / DHI `postgres:*` here; those are
    # appropriate for helper Jobs (`psqlImage`) but are not CNPG operand images.
    imageName: ghcr.io/cloudnative-pg/postgresql:18.1-system-trixie
    # Image used by GitOps helper Jobs that only need `psql` (init-roles, future
    # app db-init hooks). Prefer the Harbor proxy cache and pin by digest.
    psqlImage: harbor.m0sh1.cc/dhi/postgres:18.1-debian13@sha256:086748e4e33806af10483b2dd4bc287d7102a8cc3d11d73f5cad9886c02f3b87
    enableSuperuserAccess: true

    annotations:
      argocd.argoproj.io/sync-wave: "10"
      argocd.argoproj.io/compare-options: IgnoreExtraneous

    # Storage classes (adjust if you decide to swap)
    # Recommended default mapping given your ZFS recordsize choices:
    # - data -> nvme-fast (16K recordsize)
    # - wal  -> nvme-general (128K recordsize)
    storage:
      class: proxmox-csi-zfs-nvme-fast-retain
      size: 80Gi

    walStorage:
      class: proxmox-csi-zfs-nvme-general-retain
      size: 20Gi

    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2
        memory: 4Gi

    postgresql:
      parameters:
        max_connections: "200"
        shared_buffers: "512MB"
        # If you want WAL archiving explicitly visible in Postgres params,
        # keep these. The plugin will handle the actual archival workflow.
        archive_mode: "on"
        archive_timeout: 5min

    tolerations: []

    affinity:
      enablePodAntiAffinity: true
      topologyKey: kubernetes.io/hostname
      podAntiAffinityType: required

    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            cnpg.io/cluster: cnpg-main
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            cnpg.io/cluster: cnpg-main
    # --- Barman Cloud plugin configuration ---
    backup:
      enabled: true
      # ScheduledBackup cron (CNPG expects 6-field cron with seconds)
      schedule: "0 0 2 * * *" # Daily at 2 AM
      immediate: true

      # Name of the ObjectStore CR (templates/objectstore.yaml)
      objectStoreName: cnpg-backups

      # ObjectStore configuration (S3-compatible)
      destinationPath: s3://cnpg-backups/
      # MinIO tenant in-cluster service URL
      endpointURL: https://minio.minio-tenant.svc.cluster.local:443
      # Trust MinIO's internal TLS issuer (k3s server CA) via SealedSecret `minio-ca`.
      endpointCA:
        name: minio-ca
        key: ca.crt

      # Credentials secret (SealedSecret template keeps name/key contract)
      s3Credentials:
        accessKeyId:
          name: cnpg-backup-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: cnpg-backup-credentials
          key: ACCESS_SECRET_KEY

      # Retention is managed on the ObjectStore when using the plugin
      retentionPolicy: "30d"

      # Optional tuning passed into ObjectStore for barman-cloud behaviour
      wal:
        # zstd is supported for WAL compression
        compression: zstd
        maxParallel: 2
      data:
        # zstd is WAL-only; use a non-gzip/lz4 option for base backups
        compression: snappy
        jobs: 2

      # Optional: tune the plugin sidecar running in CNPG instance pods
      # See: https://cloudnative-pg.io/plugin-barman-cloud/docs/usage/#configuring-the-plugin-instance-sidecar
      instanceSidecarConfiguration:
        enabled: true
        retentionPolicyIntervalSeconds: 1800
        logLevel: info
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        env:
          - name: AWS_CA_BUNDLE
            value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - name: REQUESTS_CA_BUNDLE
            value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        additionalContainerArgs: []

      # One-off Backup CR (GitOps-triggered). Disable after the run completes.
      manual:
        enabled: false
        name: cnpg-main-backup-20260202-2

  # Used by databases.yaml
  databaseAnnotations:
    argocd.argoproj.io/sync-wave: "20"

  roles:
    - name: harbor
      enabled: false
      comment: Harbor application role
      login: true
      passwordSecret:
        name: harbor-postgres-auth

    - name: gitea
      enabled: false
      comment: Gitea application role
      login: true
      passwordSecret:
        name: gitea-db-secret

    - name: semaphore
      enabled: false
      comment: Semaphore application role
      login: false
      passwordSecret:
        name: semaphore-postgres-auth

    - name: authentik
      enabled: false
      comment: Authentik application role
      login: true
      passwordSecret:
        name: authentik-postgres-auth

    - name: netbox
      enabled: false
      comment: NetBox application role
      login: true
      passwordSecret:
        name: netbox-postgres-auth

    - name: uptime_kuma
      enabled: false
      comment: Uptime-Kuma application role
      login: true
      passwordSecret:
        name: uptime-kuma-postgres-auth

    - name: pgadmin
      enabled: false
      comment: PgAdmin application role
      login: true
      passwordSecret:
        name: pgadmin-postgres-auth

    - name: harborguard
      enabled: false
      comment: HarborGuard security scanning platform
      login: true
      passwordSecret:
        name: harborguard-db-secret

  databases:
    - name: harbor
      enabled: false
      owner: harbor
      databaseReclaimPolicy: retain

    - name: gitea
      enabled: false
      owner: gitea
      databaseReclaimPolicy: retain

    - name: semaphore
      enabled: false
      owner: semaphore
      databaseReclaimPolicy: retain

    - name: authentik
      enabled: false
      owner: authentik
      databaseReclaimPolicy: retain

    - name: uptime_kuma
      enabled: false
      owner: uptime_kuma
      databaseReclaimPolicy: retain

    - name: pgadmin
      enabled: false
      owner: pgadmin
      databaseReclaimPolicy: retain

    - name: harborguard
      enabled: false
      owner: harborguard
      databaseReclaimPolicy: retain

# ---- Dependency values ----

cloudnative-pg:
  config:
    clusterWide: true

  crds:
    create: true

  commonAnnotations:
    argocd.argoproj.io/sync-wave: "5"

plugin-barman-cloud:
  # Install plugin (namespaced). Typically runs in cnpg-system with the operator,
  # but it can run anywhere as long as CNPG sees the plugin capabilities.
  commonAnnotations:
    argocd.argoproj.io/sync-wave: "6"

  # Use Harbor DHI proxy cache for plugin images.
  image:
    registry: harbor.m0sh1.cc
    repository: dhi/cloudnative-pg-plugin-barman-cloud
    tag: 0.11.0-debian13@sha256:447dfcd58bda0e4034d8331d03da749665e48778e2ea347f6ffcda1a3c1dc12d

  sidecarImage:
    registry: harbor.m0sh1.cc
    repository: dhi/cloudnative-pg-plugin-barman-cloud-sidecar
    tag: 0.11.0-debian13@sha256:1a193acad4f966386b31c49493a8e95176b48752f0b1b770aa1b8a5cae9f6b90
