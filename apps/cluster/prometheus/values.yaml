kube-prometheus-stack:
  fullnameOverride: prometheus

  # Prometheus (TSDB) configuration
  prometheus:
    prometheusSpec:
      retention: 15d
      retentionSize: 20GiB
      scrapeInterval: 30s
      evaluationInterval: 30s

      # Discovery selectors
      # The chart defaults tend to restrict discovery to ServiceMonitors/Rules with a specific
      # label (e.g. Release: prometheus). Set selectors to {} to allow cross-namespace
      # discovery for additional components (e.g. Traefik), while still controlling
      # *creation* of monitors via Helm chart values in each component.
      serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false

      podMonitorSelector: {}
      podMonitorNamespaceSelector: {}
      podMonitorSelectorNilUsesHelmValues: false

      probeSelector: {}
      probeNamespaceSelector: {}
      probeSelectorNilUsesHelmValues: false

      # Keep rule discovery open as well (handy if you later add rules outside this chart)
      ruleSelector: {}
      ruleNamespaceSelector: {}

      # Leave ScrapeConfig disabled unless you also install the v1alpha1 ScrapeConfig CRD.
      # scrapeConfigSelector: {}
      # scrapeConfigNamespaceSelector: {}

      resources:
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          memory: 4Gi

      # Persist TSDB on Proxmox CSI (backed by ZFS: rpool/nvmestore on pve-01 & pve-02)
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: proxmox-csi-zfs-pgdata-retain
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi

  # Grafana (dashboards)
  grafana:
    enabled: true
    defaultDashboardsEnabled: true

    admin:
      existingSecret: grafana-admin-credentials
      userKey: admin-user
      passwordKey: admin-password

    persistence:
      enabled: true
      type: pvc
      storageClassName: proxmox-csi-zfs-pgdata-retain
      accessModes:
        - ReadWriteOnce
      size: 10Gi

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 1Gi

    deploymentStrategy:
      type: Recreate

    # We publish ingress via our Helm wrapper templates (Traefik CRDs / Ingress), not chart-managed Ingress.
    # This prevents kube-prometheus-stack from creating `Ingress observability/prometheus-grafana`.
    ingress:
      enabled: false
      hosts: []
      tls: []

    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: default
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
          - name: sidecar
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /tmp/dashboards

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: '1'
        folder: /tmp/dashboards
        provider:
          name: sidecar
          allowUiUpdates: true
        searchNamespace: ALL
      datasources:
        enabled: true
        label: grafana_datasource
        labelValue: '1'
        searchNamespace: ALL
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi
    initChownData:
      resources:
        requests:
          cpu: 25m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi
    downloadDashboards:
      resources:
        requests:
          cpu: 25m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi

    extraInitContainers:
      - name: init-mkdir-grafana-dashboards
        image: docker.io/library/busybox:1.37
        command:
          - sh
          - -c
          - |
            set -e
            mkdir -p /var/lib/grafana/dashboards/default
            mkdir -p /var/lib/grafana/dashboards/custom
        volumeMounts:
          - name: storage
            mountPath: /var/lib/grafana

    # Dashboards to import (Grafana.com IDs)
    dashboards:
      default:
        # Traefik dashboards (Grafana.com IDs)
        # NOTE: kube-prometheus-stack's Grafana dashboard downloader uses a specific *revision*.
        #   curl -fsSL https://grafana.com/api/dashboards/17347 | jq -r '.revision'
        #   curl -fsSL https://grafana.com/api/dashboards/17346 | jq -r '.revision'
        traefik-official-kubernetes:
          gnetId: 17347
          revision: 9
          datasource: Prometheus
          folder: Traefik

        traefik-official-standalone:
          gnetId: 17346
          revision: 9
          datasource: Prometheus
          folder: Traefik

        node-exporter-full:
          gnetId: 1860
          revision: 42
          datasource: Prometheus
          folder: Node Exporter

        coredns:
          gnetId: 14981
          revision: 2
          datasource: Prometheus
          folder: Kubernetes

        argocd:
          gnetId: 14584
          revision: 1
          datasource: Prometheus
          folder: Argo CD

  # Alertmanager (disabled for now)
  alertmanager:
    enabled: false

  kubeStateMetrics:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        memory: 256Mi
    tolerations:
      - operator: Exists

  nodeExporter:
    enabled: true

  prometheus-node-exporter:
    tolerations:
      - operator: Exists
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        memory: 256Mi

  # k3s note: controller-manager/scheduler/etcd run as static processes and are not reliably scrapeable
  # with the default kube-prometheus-stack ServiceMonitors unless you intentionally expose them.
  # Disable these monitors to avoid perpetual "target down" noise.
  kubeControllerManager:
    enabled: false

  kubeScheduler:
    enabled: false

  kubeEtcd:
    enabled: false

  defaultRules:
    create: true

  prometheusOperator:
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 512Mi

  # Custom alert rules (PrometheusRule) managed by kube-prometheus-stack.
  additionalPrometheusRulesMap:
    traefik:
      groups:
        - name: traefik.rules
          rules:
            # 5xx ratio across all entrypoints/routers (5m window)
            - alert: TraefikHigh5xxErrorRate
              expr: |
                (
                  sum(rate(traefik_entrypoint_requests_total{job=~"traefik.*",code=~"5.."}[5m]))
                  /
                  clamp_min(sum(rate(traefik_entrypoint_requests_total{job=~"traefik.*"}[5m])), 1)
                ) > 0.02
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: 'Traefik 5xx error rate > 2% (10m)'
                description: >-
                  Traefik is returning elevated 5xx responses. Investigate upstream services, Traefik logs, and recent deploys.
            # High request latency (P95) across all entrypoints (5m window)
            # Uses histogram buckets if present.
            - alert: TraefikHighRequestLatencyP95
              expr: |
                histogram_quantile(
                  0.95,
                  sum by (le) (rate(traefik_entrypoint_request_duration_seconds_bucket{job=~"traefik.*"}[5m]))
                ) > 0.5
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: 'Traefik request latency P95 > 0.5s (10m)'
                description: >-
                  P95 latency through Traefik is elevated. Check node pressure, upstream response times, and Traefik resource limits.

            # Traefik pods missing (works even if metrics are stale)
            - alert: TraefikPodsMissing
              expr: |
                sum(kube_pod_status_ready{namespace="traefik",condition="true"}) == 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: 'No ready Traefik pods (5m)'
                description: >-
                  No Traefik pods are Ready in namespace 'traefik'. Ingress will fail. Check Deployment/DaemonSet status, node health, and events.

            # Prometheus is not scraping Traefik (targets down)
            - alert: TraefikMetricsTargetsDown
              expr: |
                max(up{job=~"traefik.*"}) == 0
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: 'Traefik metrics targets down (10m)'
                description: >-
                  Prometheus has not been able to scrape Traefik metrics. Check ServiceMonitor, Service endpoints, NetworkPolicies, and Traefik pods.

            # Restart loop detection for Traefik pods
            - alert: TraefikPodsRestarting
              expr: |
                sum(increase(kube_pod_container_status_restarts_total{namespace="traefik"}[15m])) > 2
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: 'Traefik pods restarting'
                description: >-
                  Traefik containers are restarting. Check logs for crashes/OOM, and verify resource limits and node stability.

  # CRDs are managed separately (GitOps safe path); keep this off to avoid CRD drift/ownership
  # fights.
  crds:
    enabled: false

# Helm-wrapper values consumed by our templates/ (Traefik)
# These values are intended for apps/cluster/prometheus/templates/* (IngressRoute + Middleware).
traefik:
  enabled: false

  # Traefik entryPoint name (matches your Traefik static config)
  entryPoint: websecure

  # If you already have a wildcard TLS secret, set it here; otherwise create one via cert-manager.
  tlsSecretName: wildcard-m0sh1-cc

  grafana:
    host: grafana.m0sh1.cc
    # Default kube-prometheus-stack service name when fullnameOverride=prometheus:
    serviceName: prometheus-grafana
    servicePort: 80

  prometheus:
    host: prometheus.m0sh1.cc
    # Default kube-prometheus-stack service name when fullnameOverride=prometheus:
    serviceName: prometheus-prometheus
    servicePort: 9090

    # Recommended: protect Prometheus UI with basic-auth middleware.
    # Create the Secret via SealedSecrets:
    # - secret name: prometheus-basic-auth
    # - key: users   (htpasswd formatted)
    basicAuth:
      enabled: true
      middlewareName: prometheus-basicauth
      secretName: prometheus-basic-auth
      usersKey: users

# Host-level node_exporter scraping (systemd-managed exporters on Proxmox/K3s hosts)
hostMetrics:
  enabled: false
  port: 9100
  path: /metrics

  # Target hostnames or IPs exposed via templates/host-metrics-*.yaml
  targets:
    - 10.0.0.100 # pve-01
    - 10.0.0.101 # pve-02
    - 10.0.0.102 # pbs
    - 10.0.0.201 # lab-ctrl
    - 10.0.0.210 # horse-01
    - 10.0.0.211 # horse-02
    - 10.0.0.212 # horse-03
